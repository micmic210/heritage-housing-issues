{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Cleaning Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Evaluate missing data\n",
        "* Clean data\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/HousePrices.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate cleaned Train and Test sets, both saved under outputs/datasets/cleaned\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Change working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "Change the working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "## Load Collected Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_raw_path = \"outputs/datasets/collection/HousePrices.csv\"\n",
        "df = pd.read_csv(df_raw_path)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
        "vars_with_missing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "if vars_with_missing_data:\n",
        "    profile = ProfileReport(df=df[vars_with_missing_data], minimal=True)\n",
        "    profile.to_notebook_iframe()\n",
        "else:\n",
        "    print(\"No missing data found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation and PPS Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ppscore as pps\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Define heatmap functions\n",
        "def heatmap_corr(df, threshold, figsize=(18, 12), font_annot=8):\n",
        "    mask = np.zeros_like(df, dtype=bool)\n",
        "    mask[np.triu_indices_from(mask)] = True\n",
        "    mask[abs(df) < threshold] = True\n",
        "    \n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(df, mask=mask, annot=True, cmap=\"virids\", anoot_kws={\"size\": font_annot}, linewidths=0.5)\n",
        "    plt.show()\n",
        "\n",
        "def heatmap_pps(df, threshold, figsize=(18, 12), font_annot=8):\n",
        "    mask = np.zeros_like(df, dtype=bool)\n",
        "    mask[abs(df) < threshold] = True\n",
        "    \n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(df, mask=mask, annot=True, cmap=\"rocket_r\", annot_kws={\"size\": font_annot}, linewidths=0.5)\n",
        "    plt.show()\n",
        "\n",
        "def CalculateCorrAndPPS(df):\n",
        "    df_corr_spearman = df.corr(method=\"spearman\", numeric_only=True)\n",
        "    df_corr_pearson = df.corr(method=\"pearson\", numeric_only=True)\n",
        "\n",
        "    pps_matrix_raw = pps.matrix(df)\n",
        "    pps_matrix = pps_matrix_raw.pivot(columns='x', index='y', values='ppscore')\n",
        "\n",
        "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
        "\n",
        "# Run analysis\n",
        "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df)\n",
        "\n",
        "# Display heatmaps\n",
        "heatmap_corr(df_corr_pearson, threshold=0.4)\n",
        "heatmap_corr(df_corr_spearman, threshold=0.4)\n",
        "heatmap_pps(pps_matrix, threshold=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate Missing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def EvaluateMissingData(df):\n",
        "    missing_data_absolute = df.isnull().sum()\n",
        "    missing_data_percentage = round(missing_data_absolute / len(df) * 100, 2)\n",
        "    df_missing_data = pd.DataFrame({\n",
        "        \"RowsWithMissingData\": missing_data_absolute,\n",
        "        \"PercentageOfDataset\": missing_data_percentage,\n",
        "        \"DataType\": df.dtypes   \n",
        "    }).sort_values(by=\"PercentageOfDataset\", ascending=False).query(\"PercentageOfDataset > 0\")\n",
        "    return df_missing_data\n",
        "\n",
        "# Run on full dataset\n",
        "EvaluateMissingData(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "drop_vars = ['BsmtExposure', 'GarageFinish', 'WoodDeckSF', 'KitchenQual', 'EnclosedPorch', 'LotFrontage', 'BsmtFinType1']\n",
        "\n",
        "# Fill remaining missing numerical values (median)\n",
        "fill_median_vars = ['GarageYrBlt', 'MasVnrArea', 'BedroomAbvGr']\n",
        "\n",
        "# Fill remaining missing with Zero\n",
        "fill_zero_vars = ['2ndFlrSF']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Dataset into Train/Test "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "TrainSet, TestSet = train_test_split(df, test_size=0.2, random_state=0)\n",
        "print(f\"TrainSet: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Drop Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "\n",
        "dropper = DropFeatures(features_to_drop=drop_vars)\n",
        "dropper.fit(TrainSet)\n",
        "TrainSet = dropper.transform(TrainSet)\n",
        "TestSet = dropper.transform(TestSet)\n",
        "df = dropper.transform(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fill Zero Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col in fill_zero_vars:\n",
        "    TrainSet[col].fillna(0, inplace=True)\n",
        "    TestSet[col].fillna(0, inplace=True)\n",
        "    df[col].fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Median Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "median_imputer = SimpleImputer(strategy='median')\n",
        "TrainSet[fill_median_vars] = median_imputer.fit_transform(TrainSet[fill_median_vars])\n",
        "TestSet[fill_median_vars] = median_imputer.fit_transform(TestSet[fill_median_vars])\n",
        "df[fill_median_vars] = median_imputer.fit_transform(df[fill_median_vars])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check Final Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Missing in TrainSet:\\n\", TrainSet.isnull().sum)\n",
        "print(\"Missing in TestSet:\\n\", TestSet.isnull().sum)\n",
        "print(\"Missing in full df:\\n\", df.isnull().sum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    os.makedirs(\n",
        "        name=\"outputs/datasets/cleaned\"\n",
        "    )  # create outputs/datasets/cleaned folder\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "df.to_csv(f\"outputs/datasets/cleaned/HousePricesCleaned.csv\", index=False)\n",
        "TrainSet.to_csv(f\"outputs/datasets/cleaned/TrainSetCleaned.csv\", index=False)\n",
        "TestSet.to_csv(f\"outputs/datasets/cleaned/TestSetCleaned.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and the next steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Summary**\n",
        "\n",
        "- Loaded raw data and inspected missing values.\n",
        "- Dropped 7 low-quality or subjective features  \n",
        "  (`BsmtExposure`, `GarageFinish`, `WoodDeckSF`, `KitchenQual`, `EnclosedPorch`, `LotFrontage`, `BsmtFinType1`).\n",
        "- Imputed missing values:  \n",
        "  - `2ndFlrSF` → filled with `0`  \n",
        "  - `GarageYrBlt`, `MasVnrArea`, `BedroomAbvGr` → filled with `median`\n",
        "- Split the dataset** into Train/Test sets.\n",
        "- Confirmed that all missing values were handled.\n",
        "- Saved cleaned datasets to `outputs/datasets/cleaned`.\n",
        "\n",
        "---\n",
        "\n",
        "**Next Steps**\n",
        "\n",
        "- Create the Data Study Notebook.\n",
        "- Analyze which features most influence `SalePrice`.\n",
        "- Use visualizations such as scatter plots, box plots, and heatmaps.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
