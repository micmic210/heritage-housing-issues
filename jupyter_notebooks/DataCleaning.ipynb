{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Cleaning Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Evaluate missing data\n",
        "* Clean data\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/HousePrices.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Generate cleaned Train and Test sets, both saved under outputs/datasets/cleaned\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Change Working Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We set the parent folder as the new working directory using os.path.dirname() and os.chdir()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "## Load Collected Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"outputs/datasets/collection/HousePrices.csv\")\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When previewing the first five rows of the DataFrame above, we can already spot missing values in multiple cells across four columns. In this step, we take a closer look to identify all variables containing missing data.\n",
        "\n",
        "A total of 9 out of 24 columns—representing 75% of the dataset—contain missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
        "vars_with_missing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(vars_with_missing_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[vars_with_missing_data].info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Profile Report\n",
        "- We generate a profile report using the ydata-profiling library to examine variables with missing values in more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# adapted from customer churn study\n",
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "if vars_with_missing_data:\n",
        "    profile = ProfileReport(df=df[vars_with_missing_data], minimal=True)\n",
        "    profile.to_notebook_iframe()\n",
        "else:\n",
        "    print(\"No missing data found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We observed that several variables not only have missing values but also include many zero entries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assessing Missing Data Levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def EvaluateMissingData(df):\n",
        "    \"\"\"\n",
        "    Function to evaluate data with missing values\n",
        "    \"\"\"\n",
        "    missing_data_absolute = df.isnull().sum()\n",
        "    missing_data_percentage = round(missing_data_absolute / len(df) * 100, 2)\n",
        "    df_missing_data = (\n",
        "        pd.DataFrame(\n",
        "            data={\n",
        "                \"RowsWithMissingData\": missing_data_absolute,\n",
        "                \"PercentageOfDataset\": missing_data_percentage,\n",
        "                \"DataType\": df.dtypes,\n",
        "            }\n",
        "        )\n",
        "        .sort_values(by=[\"PercentageOfDataset\"], ascending=False)\n",
        "        .query(\"PercentageOfDataset > 0\")\n",
        "    )\n",
        "\n",
        "    return df_missing_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check missing data levels for collected dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Both EnclosedPorch and WoodDeckSF are missing over 85% of their values, so they may be dropped as they likely add little predictive value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handling Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Creating the DataCleaningEffect() function\n",
        "- This function is based on code from Unit 9 of the Feature-engine module, with some modifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def DataCleaningEffect(df_original, df_cleaned, variables_applied_with_method):\n",
        "    \"\"\"\n",
        "    Visualize the effect of data cleaning methods by comparing\n",
        "    the distributions of original and cleaned data for selected variables.\n",
        "    \"\"\"\n",
        "\n",
        "    flag_count = 1  # Track plot number\n",
        "\n",
        "    # Identify categorical variables (non-numeric types)\n",
        "    categorical_variables = df_original.select_dtypes(exclude=[\"number\"]).columns\n",
        "\n",
        "    # Display overview of analysis\n",
        "    print(\"\\n\" + \"=\" * 90)\n",
        "    print(\"* Visual Comparison of Distributions Before and After Cleaning\")\n",
        "    print(f\"* Variables assessed: {variables_applied_with_method} \\n\")\n",
        "\n",
        "    # Iterate over each selected variable\n",
        "    for var in variables_applied_with_method:\n",
        "        if var in categorical_variables:\n",
        "            # Categorical variable: bar chart comparison\n",
        "\n",
        "            df1 = pd.DataFrame({\"Type\": \"Original\", \"Value\": df_original[var]})\n",
        "            df2 = pd.DataFrame({\"Type\": \"Cleaned\", \"Value\": df_cleaned[var]})\n",
        "            df_combined = pd.concat([df1, df2], axis=0)\n",
        "\n",
        "            fig, axes = plt.subplots(figsize=(15, 5))\n",
        "            sns.countplot(\n",
        "                hue=\"Type\", data=df_combined, x=\"Value\", palette=[\"#432371\", \"#FAAE7B\"]\n",
        "            )\n",
        "            axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
        "            plt.xticks(rotation=90)\n",
        "            plt.legend()\n",
        "\n",
        "        else:\n",
        "            # Numerical variable: histogram comparison\n",
        "\n",
        "            fig, axes = plt.subplots(figsize=(10, 5))\n",
        "            sns.histplot(\n",
        "                data=df_original,\n",
        "                x=var,\n",
        "                color=\"#432371\",\n",
        "                label=\"Original\",\n",
        "                kde=True,\n",
        "                element=\"step\",\n",
        "                ax=axes,\n",
        "            )\n",
        "            sns.histplot(\n",
        "                data=df_cleaned,\n",
        "                x=var,\n",
        "                color=\"#FAAE7B\",\n",
        "                label=\"Cleaned\",\n",
        "                kde=True,\n",
        "                element=\"step\",\n",
        "                ax=axes,\n",
        "            )\n",
        "            axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
        "            plt.legend()\n",
        "\n",
        "        plt.show()\n",
        "        flag_count += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Cleaning Summary\n",
        "- Dropped Features: \n",
        "we removed EnclosedPorch and WoodDeckSF because over 80% of their values were missing. Even though they add to the size of a home, they don’t show enough variation to be useful for predictions.\n",
        "\n",
        "- Imputations \n",
        "    - Mean Imputation was used for LotFrontage and BedroomAbvGr, since their values are roughly normally distributed.\n",
        "    - Median Imputation was used for 2ndFlrSF, GarageYrBlt, and MasVnrArea. These columns are slightly skewed, and using the median helps avoid being misled by outliers.\n",
        "\n",
        "- Categorical Imputation was used for GarageFinish and BsmtFinType1 because they are text-based categories, and we can't apply mean/median to them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split Dataset into Train and Test "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We split the data so we can apply imputations on the Train Set and test their effect on the Test Set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "TrainSet, TestSet = train_test_split(df, test_size=0.2, random_state=0)\n",
        "print(f\"TrainSet: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first check missing values in the Train Set to ensure it still represents the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_missing_data = EvaluateMissingData(TrainSet)\n",
        "print(f\"* There are {df_missing_data.shape[0]} variables with missing data \\n\")\n",
        "df_missing_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Drop Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "\n",
        "dropper = DropFeatures(features_to_drop=drop_vars)\n",
        "dropper.fit(TrainSet)\n",
        "TrainSet = dropper.transform(TrainSet)\n",
        "TestSet = dropper.transform(TestSet)\n",
        "df = dropper.transform(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mean Imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Variables: ['LotFrontage' , 'BedroomAbvGr']\n",
        "- These variables have distributions that are roughly normal, so we will use the mean to fill in the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "\n",
        "# Select variables where mean is appropriate\n",
        "variables_mean = [\"LotFrontage\", \"BedroomAbvGr\"]\n",
        "\n",
        "# Create and apply the mean imputer\n",
        "imputer = MeanMedianImputer(imputation_method=\"mean\", variables=variables_mean)\n",
        "df_method = imputer.fit_transform(TrainSet)\n",
        "\n",
        "# Visualize the effect of imputation\n",
        "DataCleaningEffect(\n",
        "    df_original=TrainSet,\n",
        "    df_cleaned=df_method,\n",
        "    variables_applied_with_method=variables_mean,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Median Imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Variables: ['2ndFlrSF', 'GarageYrBlt', 'MasVnrArea']\n",
        "- These variables may have skewed distributions or outliers, so we will use the median to fill in the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "\n",
        "# Select variables where median is more robust\n",
        "variables_median = [\"2ndFlrSF\", \"GarageYrBlt\", \"MasVnrArea\"]\n",
        "\n",
        "# Create and apply the median imputer\n",
        "imputer = MeanMedianImputer(imputation_method=\"median\", variables=variables_median)\n",
        "df_method = imputer.fit_transform(TrainSet)\n",
        "\n",
        "# Visualize the effect of imputation\n",
        "DataCleaningEffect(\n",
        "    df_original=TrainSet,\n",
        "    df_cleaned=df_method,\n",
        "    variables_applied_with_method=variables_median,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After median imputation, most GarageYrBlt values are around 1975. We checked and found that when this value is missing, GarageArea is zero, meaning there's no garage. Since garage size matters more than its year, we may drop GarageYrBlt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet[(TrainSet[\"GarageArea\"] == 0)][[\"GarageYrBlt\", \"GarageArea\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical Imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Variables: ['GarageFinish', 'BsmtFinType1']\n",
        "\n",
        "- We fill missing values in these categorical columns with 'None', since they likely indicate that the garage or basement is not present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import CategoricalImputer\n",
        "\n",
        "variables_categorical = [\"GarageFinish\", \"BsmtFinType1\"]\n",
        "imputer = CategoricalImputer(\n",
        "    imputation_method=\"missing\",  \n",
        "    fill_value=\"None\",  \n",
        "    variables=variables_categorical,  \n",
        ")\n",
        "\n",
        "# Fit on training set and apply transformation\n",
        "df_method = imputer.fit_transform(TrainSet)\n",
        "\n",
        "# Visualize the impact of imputation\n",
        "DataCleaningEffect(\n",
        "    df_original=TrainSet,\n",
        "    df_cleaned=df_method,\n",
        "    variables_applied_with_method=variables_categorical,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet[(TrainSet[\"GarageArea\"] == 0)][[\"GarageFinish\", \"GarageArea\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet[(TrainSet[\"TotalBsmtSF\"] == 0)][[\"BsmtFinType1\", \"TotalBsmtSF\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning Pipeline\n",
        "- We combine all cleaning steps into one pipeline for efficiency.\n",
        "- Steps included:\n",
        "    - Mean imputation: ['LotFrontage', 'BedroomAbvGr']\n",
        "    - Median imputation: ['2ndFlrSF', 'MasVnrArea']\n",
        "    - Categorical imputation: ['GarageFinish', 'BsmtFinType1']\n",
        "    - Drop features: ['EnclosedPorch', 'GarageYrBlt', 'WoodDeckSF']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "mean_vars = [\"LotFrontage\", \"BedroomAbvGr\"]\n",
        "median_vars = [\"2ndFlrSF\", \"MasVnrArea\"]\n",
        "cat_vars = [\"GarageFinish\", \"BsmtFinType1\"]\n",
        "drop_vars = [\"EnclosedPorch\", \"GarageYrBlt\", \"WoodDeckSF\"]\n",
        "\n",
        "dataCleaning_pipeline = Pipeline(\n",
        "    [\n",
        "        (\"mean\", MeanMedianImputer(imputation_method=\"mean\", variables=mean_vars)),\n",
        "        (\n",
        "            \"median\",\n",
        "            MeanMedianImputer(imputation_method=\"median\", variables=median_vars),\n",
        "        ),\n",
        "        (\n",
        "            \"categorical\",\n",
        "            CategoricalImputer(\n",
        "                imputation_method=\"missing\", fill_value=\"None\", variables=cat_vars\n",
        "            ),\n",
        "        ),\n",
        "        (\"drop\", DropFeatures(features_to_drop=drop_vars)),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we’ll clean the full dataset by applying our pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet, TestSet = dataCleaning_pipeline.fit_transform(\n",
        "    TrainSet\n",
        "), dataCleaning_pipeline.fit_transform(TestSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = dataCleaning_pipeline.fit_transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(TrainSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(TestSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After running the missing data check, we can confirm that all missing values in the Train, Test, and original datasets have been taken care of."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    os.makedirs(\n",
        "        name=\"outputs/datasets/cleaned\"\n",
        "    )  # create outputs/datasets/cleaned folder\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "df.to_csv(f\"outputs/datasets/cleaned/HousePricesCleaned.csv\", index=False)\n",
        "TrainSet.to_csv(f\"outputs/datasets/cleaned/TrainSetCleaned.csv\", index=False)\n",
        "TestSet.to_csv(f\"outputs/datasets/cleaned/TestSetCleaned.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save Data Cleaning Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "file_path = f\"outputs/ml_pipeline/data_cleaning\"\n",
        "\n",
        "try:\n",
        "    os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(\n",
        "    value=dataCleaning_pipeline, filename=f\"{file_path}/dataCleaning_pipeline.pkl\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and the Next Steps\n",
        "\n",
        "**Summary**\n",
        "\n",
        "* Out of 24 variables, 9 (or 75%) had missing values.\n",
        "* We handled them using the following techniques:\n",
        "    - Mean imputation for: LotFrontage, BedroomAbvGr\n",
        "    - Median imputation for: 2ndFlrSF, MasVnrArea\n",
        "    - Categorical imputation for: GarageFinish, BsmtFinType1\n",
        "    - Dropped: EnclosedPorch, GarageYrBlt, WoodDeckSF\n",
        "\n",
        "**Next Steps**:\n",
        "- Create the Data Study Notebook.\n",
        "- Analyze which features most influence SalePrice.\n",
        "- Use visualizations such as scatter plots, box plots, and heatmaps.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "3.12.1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
