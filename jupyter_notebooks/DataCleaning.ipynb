{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Cleaning Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "- Evaluate missing data and outliers\n",
        "- Analyze zero vs. NaN semantics for key features\n",
        "- Quantify distributions and skewness to justify imputation methods\n",
        "- Clean data (drop, impute, handle outliers)\n",
        "- Generate cleaned Train and Test sets, saved to outputs/datasets/cleaned and export pipeline\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "- outputs/datasets/collection/HousePrices.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "- Cleaned full dataset: outputs/datasets/cleaned/HousePricesCleaned.csv\n",
        "- Cleaned train/test splits: outputs/datasets/cleaned/TrainSetCleaned.csv, outputs/datasets/cleaned/TestSetCleaned.csv\n",
        "- Data cleaning pipeline: outputs/ml_pipeline/data_cleaning/dataCleaning_pipeline.pkl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Change Working Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "os.chdir(os.path.dirname(current_dir))  # set project root\n",
        "print(\"Current directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "### Load Collected Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"outputs/datasets/collection/HousePrices.csv\")\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Identify columns with missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars_with_missing = df.columns[df.isna().sum() > 0].tolist()\n",
        "print(\"Columns with missing:\", vars_with_missing)\n",
        "print(df[vars_with_missing].info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate missing data levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def EvaluateMissingData(df):\n",
        "    missing_abs = df.isnull().sum()\n",
        "    missing_pct = (missing_abs / len(df) * 100).round(2)\n",
        "    return (\n",
        "        pd.DataFrame(\n",
        "            {\"MissingCount\": missing_abs, \"MissingPct\": missing_pct, \"DType\": df.dtypes}\n",
        "        )\n",
        "        .query(\"MissingPct > 0\")\n",
        "        .sort_values(by=\"MissingPct\", ascending=False)\n",
        "    )\n",
        "\n",
        "\n",
        "print(EvaluateMissingData(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0 vs NaN Analysis for EnclosedPorch, WoodDeckSF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col in [\"EnclosedPorch\", \"WoodDeckSF\"]:\n",
        "    zeros = (df[col] == 0).sum()\n",
        "    nans = df[col].isna().sum()\n",
        "    print(f\"{col}: zeros={zeros}, NaNs={nans}, total={len(df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpretation: If 0 entries represent \"no porch/deck\", we might keep zeros and drop NaNs separately, but since NaN missing >85%, we will drop these features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outlier Detection (IQR Method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n",
        "outliers_summary = []\n",
        "for col in numeric_cols:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
        "    outliers = df[(df[col] < lower) | (df[col] > upper)].shape[0]\n",
        "    outliers_summary.append((col, outliers))\n",
        "outliers_df = pd.DataFrame(\n",
        "    outliers_summary, columns=[\"Variable\", \"OutlierCount\"]\n",
        ").sort_values(\"OutlierCount\", ascending=False)\n",
        "outliers_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next steps: Decide later whether to Winsorize or remove specific extreme outliers in Modeling stage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Distribution & Skewness Analysis for Imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars_mean = [\"LotFrontage\", \"BedroomAbvGr\"]\n",
        "vars_median = [\"2ndFlrSF\", \"GarageYrBlt\", \"MasVnrArea\"]\n",
        "\n",
        "# Compute skewness\n",
        "skew_info = pd.Series(df[vars_mean + vars_median].skew()).to_frame(\"Skewness\")\n",
        "skew_info\n",
        "\n",
        "# Plot histograms\\import matplotlib.pyplot as plt\n",
        "for col in vars_mean + vars_median:\n",
        "    plt.figure(figsize=(6, 3))\n",
        "    df[col].hist(bins=30)\n",
        "    plt.title(f\"{col} Distribution (skew={skew_info.loc[col,'Skewness']:.2f})\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Justification:\n",
        "- vars_mean skewness near 0 ⇒ mean imputation appropriate\n",
        "- vars_median moderate skew >0.5 ⇒ median imputation more robust"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Profile Report (Missing Variables Only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "profile = ProfileReport(df[vars_with_missing], minimal=True)\n",
        "profile.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def EvaluateMissingData(df):  # (repeat for reuse)\n",
        "    missing_abs = df.isnull().sum()\n",
        "    missing_pct = (missing_abs / len(df) * 100).round(2)\n",
        "    return (\n",
        "        pd.DataFrame(\n",
        "            {\"MissingCount\": missing_abs, \"MissingPct\": missing_pct, \"DType\": df.dtypes}\n",
        "        )\n",
        "        .query(\"MissingPct > 0\")\n",
        "        .sort_values(by=\"MissingPct\", ascending=False)\n",
        "    )\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def DataCleaningEffect(df_orig, df_clean, vars_applied):\n",
        "    print(\"\\n\", \"=\" * 60)\n",
        "    for var in vars_applied:\n",
        "        plt.figure(figsize=(6, 3))\n",
        "        if df_orig[var].dtype == \"object\":\n",
        "            sns.countplot(x=var, hue=None, data=df_orig, label=\"orig\")\n",
        "            sns.countplot(x=var, hue=None, data=df_clean, label=\"clean\", alpha=0.7)\n",
        "        else:\n",
        "            sns.histplot(df_orig[var], element=\"step\", stat=\"density\", label=\"orig\")\n",
        "            sns.histplot(df_clean[var], element=\"step\", stat=\"density\", label=\"clean\")\n",
        "        plt.title(f\"{var} Before/After\")\n",
        "        plt.legend()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning Summary & Decisions\n",
        "- Drop features: EnclosedPorch, WoodDeckSF (NaN >85%)\n",
        "- Impute mean: LotFrontage, BedroomAbvGr\n",
        "- Impute median: 2ndFlrSF, GarageYrBlt, MasVnrArea\n",
        "- Impute categorical: GarageFinish, BsmtFinType1, also BsmtExposure (fill with 'None')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split Dataset into Train and Test "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "TrainSet, TestSet = train_test_split(df, test_size=0.2, random_state=0)\n",
        "print(f\"TrainSet: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify TrainSet missing before cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(EvaluateMissingData(TrainSet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Apply Data Cleaning Pipeline (separate fit/transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "from feature_engine.imputation import MeanMedianImputer, CategoricalImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Define steps\n",
        "drop_vars = [\"EnclosedPorch\", \"WoodDeckSF\"]\n",
        "mean_vars = vars_mean\n",
        "median_vars = vars_median\n",
        "cat_vars = [\"GarageFinish\", \"BsmtFinType1\", \"BsmtExposure\"]\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    [\n",
        "        (\"drop\", DropFeatures(features_to_drop=drop_vars)),\n",
        "        (\"mean\", MeanMedianImputer(imputation_method=\"mean\", variables=mean_vars)),\n",
        "        (\n",
        "            \"median\",\n",
        "            MeanMedianImputer(imputation_method=\"median\", variables=median_vars),\n",
        "        ),\n",
        "        (\n",
        "            \"cat\",\n",
        "            CategoricalImputer(\n",
        "                imputation_method=\"missing\", fill_value=\"None\", variables=cat_vars\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Fit on Train, transform both\n",
        "pipeline.fit(TrainSet)\n",
        "TrainCleaned = pipeline.transform(TrainSet)\n",
        "TestCleaned = pipeline.transform(TestSet)\n",
        "FullCleaned = pipeline.transform(df)\n",
        "\n",
        "# Check missing after cleaning\n",
        "print(\"Train missing:\\n\", EvaluateMissingData(TrainCleaned))\n",
        "print(\"Test missing:\\n\", EvaluateMissingData(TestCleaned))\n",
        "print(\"Full missing:\\n\", EvaluateMissingData(FullCleaned))\n",
        "\n",
        "# Visualize effect for each group\n",
        "DataCleaningEffect(TrainSet, TrainCleaned, mean_vars + median_vars + cat_vars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Cleaned Data and Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output dirs\n",
        "os.makedirs(\"outputs/datasets/cleaned\", exist_ok=True)\n",
        "# Save CSVs\n",
        "pd.DataFrame(FullCleaned, columns=TrainCleaned.columns).to_csv(\n",
        "    \"outputs/datasets/cleaned/HousePricesCleaned.csv\", index=False\n",
        ")\n",
        "pd.DataFrame(TrainCleaned, columns=TrainCleaned.columns).to_csv(\n",
        "    \"outputs/datasets/cleaned/TrainSetCleaned.csv\", index=False\n",
        ")\n",
        "pd.DataFrame(TestCleaned, columns=TestCleaned.columns).to_csv(\n",
        "    \"outputs/datasets/cleaned/TestSetCleaned.csv\", index=False\n",
        ")\n",
        "# Save pipeline\n",
        "import joblib\n",
        "\n",
        "os.makedirs(\"outputs/ml_pipeline/data_cleaning\", exist_ok=True)\n",
        "joblib.dump(pipeline, \"outputs/ml_pipeline/data_cleaning/dataCleaning_pipeline.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "**Summary**\n",
        "- Missing values: All handled\n",
        "- Outliers: Detected; decide handling strategy in Modeling Notebook.\n",
        "- Pipeline: Fit on train, transform test/full; avoids data leakage.\n",
        "- Justifications: Skewness, zero-vs-NaN, distribution plots, IQR analysis.\n",
        "\n",
        "**Next Steps**:\n",
        "- Move to Data Study (EDA) Notebook to analyze feature–target relationships and generate visual insights for the dashboard.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "3.12.1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
